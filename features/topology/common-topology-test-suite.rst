
---

| This file is auto-generated. Please do not edit this file, instead please edit the sources.

---

=========================================
MongoDB Driver Common Topology Test Suite
=========================================

This directory holds the **Driver Common Topology Test Suite** for MongoDB.
This is a suite of feature descriptions for MongoDB drivers according to specifications and documentation.
This suite is for behavior that depends on actual topologies,
features that are not dependent on server topology should be covered elsewhere, e.g., in unit tests.

The suite does *not* attempt exhaustive code coverage or compliance,
but should provide a reasonable balance between run time and feature testing.
We welcome improvements to the test suite.
This is *work in progress*.

This README file follows
the `MongoDB Documentation Style Guidelines <http://docs.mongodb.org/manual/meta/style-guide/>`_.
It is in `reStructuredText <http://docutils.sourceforge.net/rst.html>`_ form,
intended for `GitHub Markup <https://github.com/github/markup>`_.

Generic Tests
=============

The significant majority of tests are generic and not topology dependent.
These tests should definitely be run against a standalone **mongod** server to test basic function,
but we want to expand this so that the generic unit tests can also be run with a replica set or sharded cluster.
At present most drivers instantiate a client that connects explicitly to localhost port 27017.
This is fine for basic function,
but it makes it difficult to run generic tests against other topology configurations.
We need to do this for completeness and robustness.

The following modifications are recommended.

1. Generic tests should instantiate a client using ``MONGODB_URI`` rather than explicitly specifying localhost port 27017.
2. To cover the basic generic tests with the “standard” standalone **mongod** on localhost port 27017,
   invoke the tests with ``MONGODB_URI=’mongodb://localhost:27017’``
3. Migrate to running the generic tests against the full spectrum of “basic” preset topology configurations
   provided by `Mongo Orchestration <https://github.com/mongodb/mongo-orchestration>`_.
   Run the full generic test suite with each of the following.

   1. servers/basic.json
   2. replica_sets/basic.json
   3. sharded_clusters/basic.json

4. A test harness script that enables easy testing against a topology configuration provided by `Mongo Orchestration <https://github.com/mongodb/mongo-orchestration>`_.
   This aids both testing and development.

Generic tests should be as comprehensive as possible without being dependent on topology configuration specifics.
The generic tests should include all basic driver functions including
authorization, SSL, max values / MongoDB API version, etc.
Comprehensive generic tests are important,
as they both maximize test coverage for the above spectrum of topology configurations
and also minimize the following configuration-dependent test suit.

Topology Dependent Tests
========================

This directory holds the **Driver Common Topology Test Suite** for MongoDB
that summarizes recommended topology dependent tests.
The feature behavior is described in the `Gherkin language <https://github.com/cucumber/cucumber/wiki/Gherkin>`_
and tests can be automated using `Cucumber <http://cukes.info/>`_,
a tool for `behavior-driven development <http://en.wikipedia.org/wiki/Behavior-driven_development>`_.
Step definitions map feature steps into actual executable test code.
Ruby is used for a reference implementation of the step definitions
which is natural as Ruby is the primary implementation language for Cucumber.

We expect that driver engineers will choose to implement these topology tests as they see fit.
Cucumber can be used for actualizing the test suite,
but manual implementation of the scenarios or step definitions is also acceptable.

To support testing with various topologies,
the `mongo-orchestration <https://github.com/mongodb/mongo-orchestration>`_ project provides
for the setup, teardown, and management of topologies.
Mongo Orchestration can be easily wrapped for more streamlined use.

Reference implementations
-------------------------

Step definitions

* `step_definitions Ruby 1.x-stable
  <https://github.com/gjmurakami-10gen/mongo-ruby-driver/tree/1.x-mongo-orchestration/test/topology/step_definitions>`_

  * current execution

      $ rake test:cucumber
      ...
      46 scenarios (46 passed)
      383 steps (383 passed)
      19m37.873s

Mongo Orchestration wrapper

* `mongo_orchestration.rb Ruby 1.x-stable
  <https://github.com/gjmurakami-10gen/mongo-ruby-driver/blob/1.x-mongo-orchestration/test/orchestration/mongo_orchestration.rb>`_

Pending Feature Descriptions
============================

Incomplete but intended feature descriptions are marked ``@pending``,
mostly configuration related to replica sets or sharded clusters.

The following features are not currently in the `.feature` files.
Feature descriptions for them will be added to the `.feature` files.

Pinning
-------

* 1000 reads with nearest should all go to the same node

  * less attractive alternative - two secondaries, 1000 reads all go to the same secondary

Hidden members
--------------

* need preset configuration

  * cannot become primary, cannot read from hidden

Postponed Feature Descriptions
==============================

These feature tests are shelved and are not in the `.feature` files.

Ping Times
----------

Ping time is implementation dependent and private to the implementation.

References

* `Ping Times - Driver Read Preferences: Specification
  <https://github.com/10gen/specifications/blob/master/source/driver-read-preferences.rst#ping-times>`_
* `Drivers must not use the "ping" command - Server Discovery And Monitoring
  <https://github.com/mongodb/specifications/blob/master/source/server-discovery-and-monitoring/server-discovery-and-monitoring.rst#drivers-must-not-use-the-ping-command>`_
* `This spec does not mandate how round trip time is averaged - Server Discovery And Monitoring
  <https://github.com/mongodb/specifications/blob/master/source/server-discovery-and-monitoring/server-discovery-and-monitoring.rst#this-spec-does-not-mandate-how-round-trip-time-is-averaged>`_

Wire Protocol
-------------

References

* `Wire Protocol - 10gen / specifications
  <https://github.com/10gen/specifications/blob/master/source/driver-wire-protocol.rst>`_
* `Driver Wire Version Overlap Specification - 10gen / specifications
  <https://github.com/10gen/specifications/blob/master/source/driver-wire-version-overlap-check.rst>`_

Use the primary for write-related values and operations.

* Version
* Limits - Max Values

For adequate testing, this requires a mixed server-version replica-set topology
that is not available in mongo-orchestration.
It is shelved indefinitely.

Write Commands and Write Operations
-----------------------------------

Write operations are implemented via write commands for MongoDB version 2.6 or newer
and are implemented with the "old" wire-protocol for MongoDB version 2.4 or older.
For full spectrum testing, unit tests should be run with a matrix
that incorporates server versions
and topology categories including stand-alone server, replica set, and sharded cluster.

Testing beyond this requires a mixed server-version replica-set topology
that is not available via mongo-orchestration.

---


====================
Feature Descriptions
====================



Feature: Standalone Server Connection
=====================================

Description:

    In order to support changes to the state of a standalone server
    As a driver author
    I want to verify that the driver correctly behaves according to documentation and specification
    https://github.com/mongodb/specifications/tree/master/source/server-discovery-and-monitoring


URI:

    https://github.com/gjmurakami-10gen/mongo-meta-driver/tree/mongo-orchestration/features/topology/standalone/connection.feature



Scenario: Insert with Server Stop, Start and Restart
----------------------------------------------------


Steps:

#. **Given** a standalone server with preset basic
#. **When** I insert a document
#. **Then** the insert succeeds
#. **When** I stop the server
#. **And** I insert a document
#. **Then** the insert fails
#. **When** I start the server
#. **And** I insert a document
#. **Then** the insert succeeds
#. **When** I restart the server
#. **And** I insert a document with retries
#. **Then** the insert succeeds


Scenario: Query with Server Stop, Start and Query Auto-retry with Server Restart
--------------------------------------------------------------------------------

Description:

    See https://github.com/10gen/specifications/blob/master/source/driver-read-preferences.rst#requests-and-auto-retry
    Auto-retry - after restart, query succeeds without error/exception


Steps:

#. **Given** a standalone server with preset basic
#. **And** a document written to the server
#. **When** I query
#. **Then** the query succeeds
#. **When** I stop the server
#. **And** I query
#. **Then** the query fails
#. **When** I start the server
#. **And** I query
#. **Then** the query succeeds
#. **When** I restart the server
#. **And** I query
#. **Then** the query succeeds


Feature: Replica Set Connection
===============================

Description:

    In order to support changes to the state of a replica set
    As a driver author
    I want to verify that the driver correctly behaves according to documentation and specification
    http://docs.mongodb.org/manual/reference/command/nav-replication/
    https://github.com/mongodb/specifications/tree/master/source/server-discovery-and-monitoring


URI:

    https://github.com/gjmurakami-10gen/mongo-meta-driver/tree/mongo-orchestration/features/topology/replica_set/connection.feature



Scenario: Discovery from Primary Seed
-------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **And** I stop the arbiter
#. **And** I stop the secondary
#. **And** a replica-set client with a seed from the primary
#. **When** I query with retries and read-preference SECONDARY
#. **Then** the query succeeds
#. **When** I start the arbiter
#. **And** I query with retries and read-preference PRIMARY
#. **Then** the query succeeds
#. **When** I start the secondary
#. **And** I query with retries and read-preference SECONDARY
#. **Then** the query succeeds


Scenario: Discovery from Secondary Seed
---------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **And** I stop the arbiter
#. **And** I stop the primary
#. **And** a replica-set client with a seed from the secondary
#. **When** I query with read-preference SECONDARY
#. **Then** the query succeeds
#. **When** I start the arbiter
#. **And** I query with retries and read-preference PRIMARY
#. **Then** the query succeeds
#. **When** I start the primary
#. **And** I query with retries and read-preference SECONDARY
#. **Then** the query succeeds


Scenario: Discovery from Arbiter Seed
-------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **And** I stop the primary
#. **And** a replica-set client with a seed from the arbiter
#. **And** I query with retries and read-preference PRIMARY
#. **Then** the query succeeds
#. **When** I start the primary
#. **And** I query with retries and read-preference SECONDARY
#. **Then** the query succeeds


Scenario: Insert with Primary Step Down
---------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **When** I insert a document
#. **Then** the insert succeeds
#. **When** I command the primary to step down
#. **And** I insert a document with retries
#. **Then** the insert succeeds


Scenario: Query with Primary Step Down Query
--------------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **And** I query
#. **Then** the query succeeds
#. **When** I command the primary to step down
#. **And** I query with retries
#. **Then** the query succeeds


Scenario: Insert with Primary Stop, Start and Restart
-----------------------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **When** I insert a document
#. **Then** the insert succeeds
#. **When** I stop the primary
#. **And** I insert a document with retries
#. **Then** the insert succeeds
#. **When** I start the primary
#. **And** I insert a document with retries
#. **Then** the insert succeeds
#. **When** I restart the primary
#. **And** I insert a document with retries
#. **Then** the insert succeeds


Scenario: Query with Primary Stop, Start and Restart
----------------------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **And** I query
#. **Then** the query succeeds
#. **When** I stop the primary
#. **And** I query with retries
#. **Then** the query succeeds
#. **When** I start the primary
#. **And** I query with retries
#. **Then** the query succeeds
#. **When** I restart the primary
#. **And** I query with retries
#. **Then** the query succeeds


Feature: Read Preference
========================

Description:

    In order to support read preference that describes how clients route read operations to members of a replica set
    As a driver author
    I want to verify that the driver correctly behaves according to documentation and specification
    http://docs.mongodb.org/manual/core/read-preference/
    https://github.com/10gen/specifications/blob/master/source/driver-read-preferences.rst


URI:

    https://github.com/gjmurakami-10gen/mongo-meta-driver/tree/mongo-orchestration/features/topology/replica_set/read_preference.feature



Scenario: Read Primary
----------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **When** I track server status on all data members
#. **And** I query with read-preference PRIMARY
#. **Then** the query occurs on the primary
#. **When** there is no primary
#. **And** I query with read-preference PRIMARY
#. **Then** the query fails


Scenario: Read Primary Preferred
--------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **When** I track server status on all data members
#. **And** I query with read-preference PRIMARY_PREFERRED
#. **Then** the query occurs on the primary
#. **When** there is no primary
#. **And** I track server status on secondaries
#. **And** I query with read-preference PRIMARY_PREFERRED
#. **Then** the query occurs on the secondary


Scenario: Read Secondary
------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **When** I track server status on all data members
#. **And** I query with read-preference SECONDARY
#. **Then** the query occurs on a secondary
#. **When** there are no secondaries
#. **When** I query with read-preference SECONDARY
#. **Then** the query fails


Scenario: Read Secondary Preferred
----------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **When** I track server status on all data members
#. **And** I query with read-preference SECONDARY_PREFERRED
#. **Then** the query occurs on a secondary
#. **When** there are no secondaries
#. **And** I track server status on the primary
#. **And** I query with read-preference SECONDARY_PREFERRED
#. **Then** the query occurs on the primary


Scenario: Read With Nearest
---------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **When** I query with read-preference NEAREST
#. **Then** the query succeeds


Scenario: Read Primary With Tag Sets
------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **When** I query with read-preference PRIMARY and tag sets [{"ordinal": "one"}, {"dc": "ny"}]
#. **Then** the query fails with error "PRIMARY cannot be combined with tags"


Scenario: Read Primary Preferred With Tag Sets
----------------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **When** I track server status on all data members
#. **And** I query with read-preference PRIMARY_PREFERRED and tag sets [{"ordinal": "two"}, {"dc": "pa"}]
#. **Then** the query occurs on the primary
#. **When** there is no primary
#. **And** I track server status on secondaries
#. **And** I query with read-preference PRIMARY_PREFERRED and tag sets [{"ordinal": "two"}]
#. **Then** the query occurs on the secondary
#. **When** I query with read-preference PRIMARY_PREFERRED and tag sets [{"ordinal": "three"}, {"dc": "na"}]
#. **Then** the query fails with error "No replica set member available for query with read preference matching mode PRIMARY_PREFERRED and tags matching <tags sets>."


Scenario: Read Secondary With Tag Sets
--------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **When** I track server status on all data members
#. **And** I query with read-preference SECONDARY and tag sets [{"ordinal": "two"}]
#. **Then** the query occurs on a secondary
#. **When** I query with read-preference SECONDARY and tag sets [{"ordinal": "one"}]
#. **Then** the query fails with error "No replica set member available for query with read preference matching mode SECONDARY and tags matching <tags sets>."


Scenario: Read Secondary Preferred With Tag Sets
------------------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **When** I track server status on all data members
#. **And** I query with read-preference SECONDARY_PREFERRED and tag sets [{"ordinal": "two"}]
#. **Then** the query occurs on a secondary
#. **When** I track server status on all data members
#. **And** I query with read-preference SECONDARY_PREFERRED and tag sets [{"ordinal": "three"}]
#. **Then** the query occurs on the primary


Scenario: Read Nearest With Tag Sets
------------------------------------


Tags: `@ruby_1_x_broken`

Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **When** I track server status on all data members
#. **And** I query with read-preference NEAREST and tag sets [{"ordinal": "one"}]
#. **Then** the query occurs on the primary
#. **When** I track server status on all data members
#. **And** I query with read-preference NEAREST and tag sets [{"ordinal": "two"}]
#. **Then** the query occurs on a secondary
#. **When** I query with read-preference NEAREST and tag sets [{"ordinal": "three"}]
#. **Then** the query fails with error "No replica set member available for query with read preference matching mode NEAREST and tags matching <tags sets>"


Scenario: Secondary OK Commands
-------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **When** I track server status on all data members
#. **And** I run a <db_type> <name> command with read-preference SECONDARY and with example <example>
#. **Then** the command occurs on a <member_type>

Examples:


    | member_type | db_type | name | example | comment |
    | secondary | normal | collStats | { "collStats": "test" } |  |
    | secondary | normal | count | { "count": "test" } |  |
    | secondary | normal | dbStats | { "dbStats": 1 } |  |
    | secondary | normal | distinct | { "distinct": "test", "key": "a" } |  |
    | secondary | normal | group | { "group": { "ns": "test", "key": "a", "$reduce": "function ( curr, result ) { }", "initial": { } } } |  |
    | secondary | normal | isMaster | { "isMaster": 1 } |  |
    | secondary | normal | parallelCollectionScan | { "parallelCollectionScan": "test", "numCursors": 2 } |  |



Scenario: Secondary OK GeoNear
------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** some geo documents written to all data-bearing members
#. **And** a geo 2d index
#. **When** I track server status on all data members
#. **And** I run a geonear command with read-preference SECONDARY
#. **Then** the command occurs on a secondary


Scenario: Secondary OK GeoSearch
--------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** some geo documents written to all data-bearing members
#. **And** a geo geoHaystack index
#. **When** I track server status on all data members
#. **And** I run a geosearch command with read-preference SECONDARY
#. **Then** the command occurs on a secondary


Scenario: Secondary OK MapReduce with inline
--------------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** some documents written to all data-bearing members
#. **When** I track server status on all data members
#. **And** I run a map-reduce with field out value inline true and with read-preference SECONDARY
#. **Then** the command occurs on a secondary


Scenario: Primary Reroute MapReduce without inline
--------------------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** some documents written to all data-bearing members
#. **When** I track server status on all data members
#. **And** I run a map-reduce with field out value other than inline and with read-preference SECONDARY
#. **Then** the command occurs on the primary


Scenario: Secondary OK Aggregate without $out
---------------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** some documents written to all data-bearing members
#. **When** I track server status on all data members
#. **And** I run an aggregate without $out and with read-preference SECONDARY
#. **Then** the command occurs on a secondary


Scenario: Primary Reroute Aggregate with $out
---------------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** some documents written to all data-bearing members
#. **When** I track server status on all data members
#. **And** I run an aggregate with $out and with read-preference SECONDARY
#. **Then** the command occurs on the primary


Scenario: Primary Reroute Primary-Only Commands
-----------------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **When** I track server status on all data members
#. **And** I run a <db_type> <name> command with read-preference SECONDARY and with example <example>
#. **Then** the command occurs on the <member_type>

Examples:


    | member_type | db_type | name | example | comment |
    | primary | admin | fsync | { "fsync": 1 } |  |
    | primary | normal | ping | { "ping": 1 } |  |



Scenario: Primary Preferred Cursor Get More Continuity
------------------------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** some documents written to all data-bearing members
#. **When** I query with read-preference PRIMARY_PREFERRED and batch size 2
#. **And** I get 2 docs
#. **Then** the get succeeds
#. **When** I stop the arbiter
#. **And** I stop the primary
#. **And** I get 2 docs
#. **Then** the get fails


Scenario: Secondary Cursor Get More Continuity
----------------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** some documents written to all data-bearing members
#. **When** I query with read-preference SECONDARY and batch size 2
#. **And** I get 2 docs
#. **Then** the get succeeds
#. **When** I stop the arbiter
#. **And** I stop the primary
#. **And** I track server status on secondaries
#. **And** I get 2 docs
#. **Then** the get succeeds
#. **And** the getmore occurs on the secondary


Scenario: Secondary Kill Cursors Continuity
-------------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **And** some documents written to all data-bearing members
#. **When** I query with read-preference SECONDARY and batch size 2
#. **And** I get 2 docs
#. **Then** the get succeeds
#. **When** I stop the arbiter
#. **And** I stop the primary
#. **And** I track server status on secondaries
#. **And** I close the cursor
#. **Then** the close succeeds
#. **And** the kill cursors occurs on the secondary


Scenario: Node is unpinned upon change in read preference
---------------------------------------------------------

Description:

    See https://github.com/10gen/specifications/blob/master/source/driver-read-preferences.rst#note-on-pinning
    See https://github.com/mongodb/mongo-ruby-driver/blob/1.x-stable/test/replica_set/pinning_test.rb


Steps:

#. **Given** a replica set with preset arbiter
#. **When** I track server status on all data members
#. **And** I query with default read preference
#. **Then** the query occurs on the primary
#. **When** I track server status on all data members
#. **And** I query with read-preference SECONDARY_PREFERRED
#. **Then** the query occurs on the secondary
#. **When** I track server status on all data members
#. **And** I query with read-preference PRIMARY_PREFERRED
#. **Then** the query occurs on the primary


Scenario: Query Auto-retry with Primary Stop
--------------------------------------------

Description:

    See https://github.com/10gen/specifications/blob/master/source/driver-read-preferences.rst#requests-and-auto-retry
    Auto-retry - after primary stop, query succeeds without error/exception


Steps:

#. **Given** a replica set with preset arbiter
#. **And** a document written to all data-bearing members
#. **And** I query with read-preference PRIMARY_PREFERRED
#. **Then** the query succeeds
#. **When** I stop the primary
#. **And** I query with read-preference PRIMARY_PREFERRED
#. **Then** the query succeeds


Feature: Write Concern
======================

Description:

    In order to support write concern that describes the guarantee that
    MongoDB provides when reporting on the result of a write operation
    As a driver author
    I want to verify that the driver correctly behaves according to documentation and specification
    http://docs.mongodb.org/manual/core/write-concern/
    https://github.com/10gen/specifications/blob/master/source/driver-bulk-update.rst


URI:

    https://github.com/gjmurakami-10gen/mongo-meta-driver/tree/mongo-orchestration/features/topology/replica_set/write_concern.feature



Scenario: Write Operation with Write Concern
--------------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **When** I insert a document with the write concern {“w”: <nodes>}
#. **Then** the write operation suceeeds
#. **When** I update a document with the write concern {“w”: <nodes>}
#. **Then** the write operation suceeeds
#. **When** I delete a document with the write concern {“w”: <nodes>}
#. **Then** the write operation suceeeds


Scenario: Bulk Write Operation with Write Concern
-------------------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **When** I execute an ordered bulk write operation with the write concern {“w”: <nodes>}
#. **Then** the bulk write operation succeeds
#. **When** I remove all documents from the collection
#. **And** I execute an unordered bulk write operation with the write concern {“w”: <nodes>}
#. **Then** the bulk write operation succeeds


Scenario: Replicated Write Operations Timeout with W Failure
------------------------------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **When** I insert a document with the write concern {“w”: <nodes + 1>, “timeout”: 1}
#. **Then** the write operation fails write concern
#. **When** I update a document with the write concern {“w”: <nodes + 1>, “timeout”: 1}
#. **Then** the write operation fails write concern
#. **When** I delete a document with the write concern {“w”: <nodes + 1>, “timeout”: 1}
#. **Then** the write operation fails write concern


Scenario: Replicated Bulk Write Operation Timeout with W Failure
----------------------------------------------------------------


Steps:

#. **Given** a replica set with preset arbiter
#. **When** I execute an ordered bulk write operation with the write concern {“w”: <nodes + 1>, “timeout”: 1}
#. **Then** the bulk write operation fails
#. **And** the result includes a write concern error
#. **When** I remove all documents from the collection
#. **And** I execute an unordered bulk write operation with the write concern {“w”: <nodes + 1>, “timeout”: 1}
#. **Then** the bulk write operation fails
#. **And** the result includes a write concern error
#. **When** I remove all documents from the collection
#. **And** I execute an ordered bulk write operation with a duplicate key and with the write concern {“w”: <nodes + 1>, “timeout”: 1}
#. **Then** the bulk write operation fails
#. **And** the result includes a write error
#. **And** the result includes a write concern error
#. **When** I remove all documents from the collection
#. **And** I execute an unordered bulk write operation with a duplicate key and with the write concern {“w”: <nodes + 1>, “timeout”: 1}
#. **Then** the bulk write operation fails
#. **And** the result includes a write error
#. **And** the result includes a write concern error


Feature: Replica Set Configuration
==================================

Description:

    In order to support changes to the configuration of a replica set
    As a driver author
    I want to verify that the driver correctly behaves according to documentation and specification
    http://docs.mongodb.org/manual/reference/command/nav-replication/
    https://github.com/mongodb/specifications/tree/master/source/server-discovery-and-monitoring


URI:

    https://github.com/gjmurakami-10gen/mongo-meta-driver/tree/mongo-orchestration/features/topology/replica_set/configuration.feature



Scenario: Member is added to replica set
----------------------------------------


Tags: `@pending`


Scenario: Member is removed from replica set
--------------------------------------------


Tags: `@pending`


Feature: Sharded Cluster Connection
===================================

Description:

    In order to support changes to the state of a sharded cluster
    As a driver author
    I want to verify that the driver correctly behaves according to documentation and specification
    https://github.com/mongodb/specifications/tree/master/source/server-discovery-and-monitoring


URI:

    https://github.com/gjmurakami-10gen/mongo-meta-driver/tree/mongo-orchestration/features/topology/sharded_cluster/connection.feature



Scenario: Insert with mongos Router Stop and Start
--------------------------------------------------


Steps:

#. **Given** a sharded cluster with preset basic
#. **When** I insert a document
#. **Then** the insert succeeds
#. **When** I stop router A
#. **And** I insert a document with retries
#. **Then** the insert succeeds
#. **When** I stop router B
#. **And** I insert a document
#. **Then** the insert fails
#. **When** I start router B
#. **And** I insert a document
#. **Then** the insert succeeds
#. **When** I start router A
#. **And** I insert a document
#. **Then** the insert succeeds
#. **When** I stop router B
#. **And** I insert a document with retries
#. **Then** the insert succeeds


Scenario: Query Auto-retry with mongos Router Stop and Start
------------------------------------------------------------

Description:

    See https://github.com/10gen/specifications/blob/master/source/driver-read-preferences.rst#requests-and-auto-retry
    Auto-retry - mongos fail-over - query succeeds without error/exception as long as one mongos is available


Steps:

#. **Given** a sharded cluster with preset basic
#. **And** a document written to the cluster
#. **When** I query
#. **Then** the query succeeds
#. **When** I stop router A
#. **When** I query
#. **Then** the query succeeds
#. **When** I stop router B
#. **When** I query
#. **Then** the query fails
#. **When** I start router B
#. **When** I query
#. **Then** the query succeeds
#. **When** I start router A
#. **When** I query
#. **Then** the query succeeds
#. **When** I stop router B


Scenario: Insert with mongos Router Restart
-------------------------------------------


Steps:

#. **Given** a sharded cluster with preset basic
#. **When** I insert a document
#. **Then** the insert succeeds
#. **When** I stop router A
#. **And** I insert a document with retries
#. **Then** the insert succeeds
#. **When** I restart router B
#. **And** I insert a document with retries
#. **Then** the insert succeeds


Scenario: Query Auto-retry with mongos Router Restart
-----------------------------------------------------

Description:

    See https://github.com/10gen/specifications/blob/master/source/driver-read-preferences.rst#requests-and-auto-retry
    Auto-retry - mongos fail-over - query succeeds without error/exception as long as one mongos is available


Steps:

#. **Given** a sharded cluster with preset basic
#. **And** a document written to the cluster
#. **When** I query
#. **Then** the query succeeds
#. **When** I stop router A
#. **And** I query
#. **Then** the query succeeds
#. **When** I restart router B
#. **And** I query
#. **Then** the query succeeds


Feature: Sharded Cluster Configuration
======================================

Description:

    In order to support changes to the configuration of a sharded cluster
    As a driver author
    I want to verify that the driver correctly behaves according to documentation and specification
    http://docs.mongodb.org/manual/reference/command/nav-sharding/
    http://docs.mongodb.org/manual/reference/command/nav-replication/
    https://github.com/mongodb/specifications/tree/master/source/server-discovery-and-monitoring


URI:

    https://github.com/gjmurakami-10gen/mongo-meta-driver/tree/mongo-orchestration/features/topology/sharded_cluster/configuration.feature



Scenario: Router added to cluster
---------------------------------


Tags: `@pending`


Scenario: Router removed from cluster
-------------------------------------


Tags: `@pending`


Scenario: Shard added to cluster
--------------------------------


Tags: `@pending`


Scenario: Shard removed from cluster
------------------------------------


Tags: `@pending`

---

| This file is auto-generated. Please do not edit this file, instead please edit the sources.

---

